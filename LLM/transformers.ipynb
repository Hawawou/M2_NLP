{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = AutoModel.from_pretrained('Qwen/Qwen2.5-0.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight torch.Size([151936, 896])\n",
      "layers.0.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.0.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.0.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.0.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.0.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.0.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.0.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.0.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.0.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.0.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.0.input_layernorm.weight torch.Size([896])\n",
      "layers.0.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.1.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.1.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.1.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.1.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.1.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.1.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.1.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.1.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.1.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.1.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.1.input_layernorm.weight torch.Size([896])\n",
      "layers.1.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.2.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.2.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.2.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.2.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.2.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.2.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.2.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.2.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.2.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.2.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.2.input_layernorm.weight torch.Size([896])\n",
      "layers.2.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.3.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.3.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.3.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.3.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.3.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.3.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.3.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.3.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.3.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.3.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.3.input_layernorm.weight torch.Size([896])\n",
      "layers.3.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.4.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.4.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.4.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.4.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.4.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.4.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.4.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.4.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.4.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.4.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.4.input_layernorm.weight torch.Size([896])\n",
      "layers.4.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.5.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.5.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.5.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.5.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.5.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.5.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.5.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.5.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.5.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.5.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.5.input_layernorm.weight torch.Size([896])\n",
      "layers.5.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.6.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.6.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.6.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.6.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.6.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.6.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.6.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.6.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.6.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.6.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.6.input_layernorm.weight torch.Size([896])\n",
      "layers.6.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.7.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.7.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.7.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.7.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.7.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.7.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.7.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.7.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.7.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.7.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.7.input_layernorm.weight torch.Size([896])\n",
      "layers.7.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.8.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.8.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.8.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.8.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.8.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.8.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.8.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.8.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.8.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.8.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.8.input_layernorm.weight torch.Size([896])\n",
      "layers.8.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.9.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.9.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.9.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.9.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.9.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.9.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.9.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.9.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.9.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.9.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.9.input_layernorm.weight torch.Size([896])\n",
      "layers.9.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.10.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.10.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.10.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.10.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.10.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.10.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.10.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.10.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.10.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.10.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.10.input_layernorm.weight torch.Size([896])\n",
      "layers.10.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.11.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.11.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.11.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.11.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.11.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.11.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.11.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.11.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.11.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.11.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.11.input_layernorm.weight torch.Size([896])\n",
      "layers.11.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.12.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.12.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.12.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.12.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.12.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.12.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.12.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.12.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.12.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.12.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.12.input_layernorm.weight torch.Size([896])\n",
      "layers.12.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.13.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.13.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.13.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.13.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.13.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.13.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.13.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.13.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.13.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.13.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.13.input_layernorm.weight torch.Size([896])\n",
      "layers.13.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.14.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.14.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.14.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.14.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.14.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.14.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.14.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.14.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.14.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.14.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.14.input_layernorm.weight torch.Size([896])\n",
      "layers.14.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.15.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.15.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.15.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.15.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.15.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.15.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.15.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.15.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.15.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.15.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.15.input_layernorm.weight torch.Size([896])\n",
      "layers.15.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.16.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.16.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.16.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.16.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.16.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.16.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.16.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.16.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.16.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.16.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.16.input_layernorm.weight torch.Size([896])\n",
      "layers.16.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.17.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.17.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.17.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.17.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.17.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.17.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.17.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.17.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.17.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.17.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.17.input_layernorm.weight torch.Size([896])\n",
      "layers.17.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.18.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.18.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.18.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.18.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.18.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.18.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.18.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.18.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.18.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.18.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.18.input_layernorm.weight torch.Size([896])\n",
      "layers.18.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.19.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.19.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.19.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.19.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.19.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.19.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.19.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.19.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.19.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.19.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.19.input_layernorm.weight torch.Size([896])\n",
      "layers.19.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.20.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.20.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.20.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.20.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.20.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.20.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.20.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.20.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.20.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.20.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.20.input_layernorm.weight torch.Size([896])\n",
      "layers.20.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.21.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.21.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.21.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.21.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.21.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.21.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.21.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.21.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.21.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.21.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.21.input_layernorm.weight torch.Size([896])\n",
      "layers.21.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.22.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.22.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.22.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.22.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.22.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.22.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.22.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.22.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.22.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.22.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.22.input_layernorm.weight torch.Size([896])\n",
      "layers.22.post_attention_layernorm.weight torch.Size([896])\n",
      "layers.23.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "layers.23.self_attn.q_proj.bias torch.Size([896])\n",
      "layers.23.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "layers.23.self_attn.k_proj.bias torch.Size([128])\n",
      "layers.23.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "layers.23.self_attn.v_proj.bias torch.Size([128])\n",
      "layers.23.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "layers.23.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "layers.23.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "layers.23.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "layers.23.input_layernorm.weight torch.Size([896])\n",
      "layers.23.post_attention_layernorm.weight torch.Size([896])\n",
      "norm.weight torch.Size([896])\n"
     ]
    }
   ],
   "source": [
    "for n, p in mod.named_parameters():\n",
    "    print(n, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 24 layers\n",
    "- 151936 input tokens\n",
    "- 869: size of each vectors. Hidden size of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling: Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Qwen2Model is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m     final_response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(model\u001b[38;5;241m=\u001b[39mmod, messages\u001b[38;5;241m=\u001b[39mmessages)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(final_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 85\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m---> 31\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m          \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgetnews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGet recent news from a country\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcountry\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mThe name of the country\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrequired\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcountry\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m              \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m          \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Add the model's response to the conversation history\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/School/.venv/lib/python3.12/site-packages/ollama/_client.py:236\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    234\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/School/.venv/lib/python3.12/site-packages/ollama/_client.py:99\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[0;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m     96\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m---> 99\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/School/.venv/lib/python3.12/site-packages/ollama/_client.py:70\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m---> 70\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/School/.venv/lib/python3.12/site-packages/httpx/_client.py:824\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    817\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting per-request cookies=<...> is being deprecated, because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    819\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe expected behaviour on cookie persistence is ambiguous. Set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcookies directly on the client instance instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m     )\n\u001b[1;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m--> 824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[0;32m~/School/.venv/lib/python3.12/site-packages/httpx/_client.py:358\u001b[0m, in \u001b[0;36mBaseClient.build_request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, timeout, extensions)\u001b[0m\n\u001b[1;32m    352\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, UseClientDefault)\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m Timeout(timeout)\n\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    357\u001b[0m     extensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextensions, timeout\u001b[38;5;241m=\u001b[39mtimeout\u001b[38;5;241m.\u001b[39mas_dict())\n\u001b[0;32m--> 358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/School/.venv/lib/python3.12/site-packages/httpx/_models.py:342\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[0;34m(self, method, url, params, headers, cookies, content, data, files, json, stream, extensions)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     content_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 342\u001b[0m     headers, stream \u001b[38;5;241m=\u001b[39m \u001b[43mencode_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mboundary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_multipart_boundary_from_content_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontent_type\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare(headers)\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m~/School/.venv/lib/python3.12/site-packages/httpx/_content.py:214\u001b[0m, in \u001b[0;36mencode_request\u001b[0;34m(content, data, files, json, boundary)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encode_urlencoded_data(data)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m json \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencode_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {}, ByteStream(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/School/.venv/lib/python3.12/site-packages/httpx/_content.py:177\u001b[0m, in \u001b[0;36mencode_json\u001b[0;34m(json)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_json\u001b[39m(json: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], ByteStream]:\n\u001b[0;32m--> 177\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[43mjson_dumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     content_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(body))\n\u001b[1;32m    179\u001b[0m     content_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.12/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m/usr/lib/python3.12/json/encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/usr/lib/python3.12/json/encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Qwen2Model is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'What is the main news right now in the USA?'}]\n",
    "\n",
    "def getnews(c):\n",
    "    c=c.lower().strip()\n",
    "    if c=='france': s='fr'\n",
    "    elif c=='india': s='in'\n",
    "    elif c=='usa': s='us'\n",
    "    elif c=='australia': s='au'\n",
    "    elif c=='russia': s='ru'\n",
    "    elif c=='united kingdom': s='gb'\n",
    "    else:\n",
    "        print(\"unknown country\",c)\n",
    "        s='fr'\n",
    "    url=\"https://saurav.tech/NewsAPI/top-headlines/category/general/\"+s+\".json\"\n",
    "    print(\"calling fct\")\n",
    "    response = requests.get(url)\n",
    "    res = response.text\n",
    "    print(\"tool res\",res)\n",
    "    print(\"\\n\"*5)\n",
    "\n",
    "    n=json.loads(res)\n",
    "    r=n['articles'][0]['title']+\": \"+n['articles'][0]['content']\n",
    "    print(\"extracting news\",r,\"\\n\"*3)\n",
    "    return r\n",
    "\n",
    "def main():\n",
    "    response = ollama.chat(\n",
    "        model=mod,\n",
    "        messages=messages,\n",
    "        tools=[\n",
    "          {\n",
    "            'type': 'function',\n",
    "            'function': {\n",
    "              'name': 'getnews',\n",
    "              'description': 'Get recent news from a country',\n",
    "              'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'country': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'The name of the country',\n",
    "                        },\n",
    "                },\n",
    "                'required': ['country'],\n",
    "              },\n",
    "            },\n",
    "          },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Add the model's response to the conversation history\n",
    "    messages.append(response['message'])\n",
    "    print(\"first answer\",response['message'])\n",
    "\n",
    "    # Check if the model decided to use the provided function\n",
    "    if not response['message'].get('tool_calls'):\n",
    "        print(\"The model didn't use the function. Its response was:\")\n",
    "        print(response['message']['content'])\n",
    "        return\n",
    "\n",
    "    # Process function calls made by the model\n",
    "    if response['message'].get('tool_calls'):\n",
    "        available_functions = {\n",
    "          'getnews': getnews,\n",
    "        }\n",
    "        for tool in response['message']['tool_calls']:\n",
    "          function_to_call = available_functions[tool['function']['name']]\n",
    "          function_response = function_to_call(tool['function']['arguments']['country'])\n",
    "          # Add function response to the conversation\n",
    "          messages.append(\n",
    "            {\n",
    "              'role': 'tool',\n",
    "              'content': function_response,\n",
    "            }\n",
    "          )\n",
    "\n",
    "    # Second API call: Get final response from the model\n",
    "    final_response = ollama.chat(model=mod, messages=messages)\n",
    "    print(final_response['message']['content'])\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
