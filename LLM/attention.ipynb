{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random tensor\n",
    "X1 = torch.rand()\n",
    "X2 = torch.rand()\n",
    "X3 = torch.rand()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC 61.165432 0.530000\n",
      "ACC 47.665056 0.870000\n",
      "ACC 38.102483 0.980000\n",
      "ACC 30.837838 0.990000\n",
      "ACC 25.354845 0.990000\n",
      "ACC 21.197354 0.990000\n",
      "ACC 18.006459 0.990000\n",
      "ACC 15.518390 0.990000\n",
      "ACC 13.545126 0.990000\n",
      "ACC 11.953592 1.000000\n",
      "ACC 10.648970 1.000000\n",
      "ACC 9.562880 1.000000\n",
      "ACC 8.645409 1.000000\n",
      "ACC 7.860019 1.000000\n",
      "ACC 7.180101 1.000000\n",
      "ACC 6.586386 1.000000\n",
      "ACC 6.064934 1.000000\n",
      "ACC 5.605268 1.000000\n",
      "ACC 5.198860 1.000000\n",
      "ACC 4.838309 1.000000\n",
      "ACC 4.517059 1.000000\n",
      "ACC 4.229427 1.000000\n",
      "ACC 3.970605 1.000000\n",
      "ACC 3.736593 1.000000\n",
      "ACC 3.524068 1.000000\n",
      "ACC 3.330263 1.000000\n",
      "ACC 3.152874 1.000000\n",
      "ACC 2.989956 1.000000\n",
      "ACC 2.839862 1.000000\n",
      "ACC 2.701185 1.000000\n",
      "ACC 2.572716 1.000000\n",
      "ACC 2.453416 1.000000\n",
      "ACC 2.342374 1.000000\n",
      "ACC 2.238793 1.000000\n",
      "ACC 2.141976 1.000000\n",
      "ACC 2.051308 1.000000\n",
      "ACC 1.966251 1.000000\n",
      "ACC 1.886322 1.000000\n",
      "ACC 1.811092 1.000000\n",
      "ACC 1.740181 1.000000\n",
      "ACC 1.673240 1.000000\n",
      "ACC 1.609967 1.000000\n",
      "ACC 1.550080 1.000000\n",
      "ACC 1.493330 1.000000\n",
      "ACC 1.439492 1.000000\n",
      "ACC 1.388358 1.000000\n",
      "ACC 1.339744 1.000000\n",
      "ACC 1.293476 1.000000\n",
      "ACC 1.249403 1.000000\n",
      "ACC 1.207381 1.000000\n",
      "ACC 1.167279 1.000000\n",
      "ACC 1.128977 1.000000\n",
      "ACC 1.092365 1.000000\n",
      "ACC 1.057343 1.000000\n",
      "ACC 1.023815 1.000000\n",
      "ACC 0.991696 1.000000\n",
      "ACC 0.960904 1.000000\n",
      "ACC 0.931367 1.000000\n",
      "ACC 0.903015 1.000000\n",
      "ACC 0.875784 1.000000\n",
      "ACC 0.849615 1.000000\n",
      "ACC 0.824451 1.000000\n",
      "ACC 0.800242 1.000000\n",
      "ACC 0.776935 1.000000\n",
      "ACC 0.754491 1.000000\n",
      "ACC 0.732863 1.000000\n",
      "ACC 0.712012 1.000000\n",
      "ACC 0.691903 1.000000\n",
      "ACC 0.672497 1.000000\n",
      "ACC 0.653764 1.000000\n",
      "ACC 0.635673 1.000000\n",
      "ACC 0.618193 1.000000\n",
      "ACC 0.601299 1.000000\n",
      "ACC 0.584965 1.000000\n",
      "ACC 0.569166 1.000000\n",
      "ACC 0.553878 1.000000\n",
      "ACC 0.539080 1.000000\n",
      "ACC 0.524754 1.000000\n",
      "ACC 0.510879 1.000000\n",
      "ACC 0.497446 1.000000\n",
      "ACC 0.486303 1.000000\n",
      "ACC 0.472208 1.000000\n",
      "ACC 0.459416 1.000000\n",
      "ACC 0.447548 1.000000\n",
      "ACC 0.436053 1.000000\n",
      "ACC 0.424903 1.000000\n",
      "ACC 0.414080 1.000000\n",
      "ACC 0.403575 1.000000\n",
      "ACC 0.393376 1.000000\n",
      "ACC 0.383473 1.000000\n",
      "ACC 0.373859 1.000000\n",
      "ACC 0.364520 1.000000\n",
      "ACC 0.355451 1.000000\n",
      "ACC 0.346641 1.000000\n",
      "ACC 0.338084 1.000000\n",
      "ACC 0.329770 1.000000\n",
      "ACC 0.321695 1.000000\n",
      "ACC 0.313848 1.000000\n",
      "ACC 0.306223 1.000000\n",
      "ACC 0.298814 1.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n=1 # input (X) dim\n",
    "m=5 # Q,K,V dim\n",
    "\n",
    "class SelfAtt(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SelfAtt,self).__init__()\n",
    "        wwq = torch.rand(n,m)\n",
    "        wwk = torch.rand(n,m)\n",
    "        wwv = torch.rand(n,m)\n",
    "        self.wq = torch.nn.Parameter(wwq)\n",
    "        self.wk = torch.nn.Parameter(wwk)\n",
    "        self.wv = torch.nn.Parameter(wwv)\n",
    "        self.fc = torch.nn.Linear(m,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x = (B=T,n)\n",
    "        # print(\"x\",x.shape)\n",
    "        keys = torch.matmul(x,self.wk)\n",
    "        # print(\"k\",keys.shape)\n",
    "        queries = torch.matmul(x,self.wq)\n",
    "        # print(\"q\",keys.shape)\n",
    "        values = torch.matmul(x,self.wv)\n",
    "\n",
    "        att = torch.matmul(queries,keys.transpose(0,1))\n",
    "        # print(\"att\",att.shape)\n",
    "        s = torch.nn.Softmax(dim=1)\n",
    "        natt = s(att)\n",
    "\n",
    "        t = x.size(0)\n",
    "        # print(\"natt\",natt.shape)\n",
    "        alpha = natt.unsqueeze(-1).expand(t,t,m)\n",
    "        # print(\"alpha\",alpha.shape)\n",
    "        weighted = torch.mul(values,alpha)\n",
    "        zatt = weighted.sum(dim=1)\n",
    "\n",
    "        z = zatt.max(dim=0)[0]\n",
    "        y = self.fc(z)\n",
    "        return y\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "# create various types of artificial data to test self-attention\n",
    "if True:\n",
    "    for i in range(100):\n",
    "        t = 1+int(torch.rand(1,).item()*10.)\n",
    "        x = torch.rand(t+t,1)\n",
    "        xs.append(x)\n",
    "        if torch.rand(1,).item()<0.5:\n",
    "            for j in range(t+t): x[j] += 1.\n",
    "            ys.append(torch.LongTensor([0]))\n",
    "        else:\n",
    "            ys.append(torch.LongTensor([1]))\n",
    "else:\n",
    "    for i in range(100):\n",
    "        t = 1+int(torch.rand(1,).item()*10.)\n",
    "        x = torch.rand(t+t,1)\n",
    "        xs.append(x)\n",
    "        if torch.rand(1,).item()<0.5:\n",
    "            for j in range(t): x[t+j] += 1.\n",
    "            ys.append(torch.LongTensor([0]))\n",
    "        else:\n",
    "            for j in range(t): x[j] += 1.\n",
    "            ys.append(torch.LongTensor([1]))\n",
    "\n",
    "\n",
    "mod=SelfAtt()\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(mod.parameters(),lr=0.01)\n",
    "for ep in range(100):\n",
    "    lo = 0.\n",
    "    nok=0\n",
    "    for i in range(len(xs)):\n",
    "        opt.zero_grad()\n",
    "        y = mod(xs[i])\n",
    "        if y.max(dim=0)[1]==ys[i]: nok+=1\n",
    "        y = y.view(1,-1)\n",
    "        los = loss(y,ys[i])\n",
    "        los.backward()\n",
    "        opt.step()\n",
    "        lo += los.item()\n",
    "    acc = float(nok)/float(len(xs))\n",
    "    print(\"ACC %f %f\" % (lo,acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
