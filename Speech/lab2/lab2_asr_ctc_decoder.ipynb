{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognitionwith CTC Decoder\n",
    "\n",
    "**Name**:\n",
    "\n",
    "The first lab introduced the building blocks of an ASR system, including feature extraction and classification with an acoustic model (wav2vec2), which produced an *emission* matrix (probability for each character at each time step). From this emission matrix, we could compute the most likely character at each time step using a naïve *greedy* decoder. The drawback of such an approach is the lack of context, which can produce sequences of characters that do not correspond to actual words, and/or sequences of words that are incorrect / do not correspond to any language rules.\n",
    "\n",
    "In this lab, we introduce the usage of a more advanced decoding technique that is based on [connectionist temporal classification](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c) (CTC). The general idea of such a decoder is to consider some context (sequences of characters, possible words, and possible sequences of words), in oder to yield more likely / realistic outputs than those given by the greedy decoder.\n",
    "\n",
    "<center><a href=\"https://gab41.lab41.org/speech-recognition-you-down-with-ctc-8d3b558943f0\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*XbIp4pn38rxL_DwzSeYVxQ.png\" width=\"400\"></a></center>\n",
    "\n",
    "To do so, the CTC decoder relies on three main components:\n",
    "    \n",
    "- A **beam search**, which is an algorithm to efficently find the *best path* from the emission matrix, that is, the sequence of characters with highest probability (rather than the sequence of individually most likely characters).\n",
    "- A **lexicon**, which is a mapping between token sequences (list of characters) and words. It is used to restrict the search space of the decoder to words that only belong to this dictionary (e.g., the word \"azfpojazflj\" does not exist in the English vocabulary).\n",
    "- A **language model**, which specifies sequences of words that are more likely to occur than others. A common choice of language model is an $n$-gram, which is a statistical model for the probability of occurrence of a given word based on the previous $n$ ones (for instance, the sequence \"the sky is\" is more likely to be followed with \"blue\" rather than \"trumpet\").\n",
    "\n",
    "The CTC decoder combines these ingredients to compute the score of several word sequences (or *hypothesis*), in order to find the best possible transcript. In this lab, we study the influence of the lexicon, language model, and the beam search size onto ASR performance from a practical perspective, without going into the technical details of the [beam search algorithm](https://www.width.ai/post/what-is-beam-search) or the [CTC loss](https://distill.pub/2017/ctc/) (which can also be used for training the network). \n",
    "\n",
    "**Note**: This lab is based on this [tutorial](https://pytorch.org/audio/main/tutorials/asr_inference_with_ctc_decoder_tutorial.html), which you can check for more details on CTC decoder parameters in torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.models.decoder import ctc_decoder, download_pretrained_files\n",
    "import IPython\n",
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "MAX_FILES = 100 # lower this number for processing a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset path - Only change it HERE if needed and not later in the notebook\n",
    "data_dir = \"asr-dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "As in the previous lab, we first load an example speech signal, and we display it. We also provide the function to get the true transcript and compute the WER. Finally, we load the wav2vec2 acoustic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path (audio and transcripts)\n",
    "data_speech_dir = os.path.join(data_dir, 'speech')\n",
    "data_transc_dir = os.path.join(data_dir, 'transcription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example file\n",
    "audio_file = '61-70968-0001.wav'\n",
    "audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "print(f\"Audio file path: {audio_file_path}\")\n",
    "\n",
    "waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "IPython.display.Audio(data=waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide the function for loading the true transcript and computing the WER\n",
    "def get_true_transcript(transc_file_path):\n",
    "    with open(transc_file_path, \"r\") as f:\n",
    "        true_transcript = f.read()\n",
    "    true_transcript = true_transcript.lower().split()\n",
    "    return true_transcript\n",
    "\n",
    "def get_wer(true_transcript, est_transcript):\n",
    "    wer = torchaudio.functional.edit_distance(true_transcript, est_transcript) / len(true_transcript)\n",
    "    return wer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the true transcription\n",
    "transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "true_transcript = get_true_transcript(transc_file_path)\n",
    "print(true_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the acoustic model\n",
    "model_name = 'WAV2VEC2_ASR_BASE_100H'\n",
    "bundle = getattr(torchaudio.pipelines, model_name)\n",
    "acoustic_model = bundle.get_model()\n",
    "labels = bundle.get_labels()\n",
    "\n",
    "# Apply the model to the waveform to get the emission tensor\n",
    "with torch.inference_mode():\n",
    "    emission, _ = acoustic_model(waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTC Decoder\n",
    "\n",
    "The CTC decoder can be constructed directly by using the `ctc_decoder` function in torchaudio. In addition to the parameters related to the beam search (we will study them later on), it takes as inputs:\n",
    "- the list of tokens, in order to map emissions to characters in the classifier.\n",
    "- the path to the lexicon, expected as a .txt file containing, on each line, a word followed by its space-split tokens (and special end-of-sequence token `|`):\n",
    "\n",
    "```\n",
    "# lexicon.txt\n",
    "a     a |\n",
    "able  a b l e |\n",
    "about a b o u t |\n",
    "...\n",
    "```\n",
    "- the path to the language model, expected as a .bin file.\n",
    "\n",
    "All these are assembled in pretrained files that can be downloaded using the `download_pretrained_files` function (this might take some time as the language model can be large), and then used to contruct the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download the files corresponding to the model: here we use a 4-gram language model (which comes with lexicon and tokens)\n",
    "files = download_pretrained_files(\"librispeech-4-gram\")\n",
    "print(files.tokens)\n",
    "print(files.lexicon)\n",
    "print(files.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize the first 10 tokens (includes the blank and end-of-word token)\n",
    "with open(files.tokens, 'r') as f:\n",
    "    tok = f.read().splitlines()\n",
    "print(\"\\n\".join(tok[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize the lexicon content (first 10 entries)\n",
    "with open(files.lexicon, 'r') as f:\n",
    "    lex = f.read().splitlines()\n",
    "print(\"\\n\".join(lex[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate the CTC decoder\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=files.lexicon,\n",
    "    tokens=files.tokens,\n",
    "    lm=files.lm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the constructed decoder to the `emission` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the decoder, and get the first element (batch_size=1) and best hypothesis\n",
    "ctc_decoder_result = decoder(emission)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the transcript\n",
    "\n",
    "The decoder output `ctc_decoder_result` contains many fields, including the predicted token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the token IDs using the .tokens field\n",
    "ctc_decoder_indices = ctc_decoder_result.tokens\n",
    "print(f\"Token indices: {ctc_decoder_indices}\")\n",
    "\n",
    "# You can manually convert token IDs to tokens using the decoder.idxs_to_tokens method\n",
    "# (+ a bit of postprocessing)\n",
    "ctc_decoder_tokens = decoder.idxs_to_tokens(ctc_decoder_indices)\n",
    "ctc_decoder_transcript = \"\".join(ctc_decoder_tokens).replace(\"|\", \" \").strip().split() \n",
    "\n",
    "print(f\"Transcript: {ctc_decoder_transcript}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can obtain the transcript directly via the `.words` field, if a lexicon is provided; otherwise the `.words` field is an empty list, so the transcript needs to be built manually as done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Words: {ctc_decoder_result.words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the lexicon\n",
    "\n",
    "The lexicon is expected to have a strong influence on ASR performance, since it constrains the decoder to produce only words that belong to this lexicon, therefore avoiding to procude words that potentially do not exist in a language or given corpus.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 1**</span>. Create your own custom lexicon file `mylexicon.txt` that contains only words that belong to the example audio sentence (\"give\", \"not\", \"so\", etc.). Construct a decoder using this lexicon (as well as the language model) and perform ASR. Display the result and compare it with the true transcript. Does it seem relevent to use such a specific lexicon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the language model\n",
    "\n",
    "The language model is also expected to have a strong impact onto performance, since it guides the decoder towards more likely word sequences.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 2**</span>. Construct a decoder with no language model nor lexicon (pass `None` as input arguments). Perform ASR on the example audio and display the transcript. Compare it with the transcript obtained using the greedy decoder in the first lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 3**</span>. Re-construct a CTC decoder using `librispeech-4-gram` files (lexicon, token, and language model downloaded above in this script). Perform ASR on the whole dataset folder (feel free to reuse/adapt a function from the previous lab) and compute the mean WER. Compare it with the WER when performing ASR with the greedy decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search parameters\n",
    "\n",
    "The beam search algorithm used in the CTC decoder depends on other parameters, such as `nbest` which determines the number of hypotheses to return, or `lm_weight` which adjust the relative importance of the language model vs. the acoustic model predictions. Here we only focus on `beam_size`, which determines the maximum number of best hypotheses to hold after each decoding step. Using larger beam sizes allows for exploring a larger range of possible hypotheses which can produce hypotheses with higher scores, which really is [the core](https://distill.pub/2017/ctc/) of the beam search algorithm.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 4**</span>. Perform ASR on the whole dataset folder for several values of the beam search size parameter: `beam_size` $\\in [1, 10, 100]$. Compute the WER and the computation time (e.g., via the [time](https://docs.python.org/3/library/time.html#time.time) package). Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
