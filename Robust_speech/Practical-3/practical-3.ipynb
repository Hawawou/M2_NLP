{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 3 - \n",
    "\n",
    "[Nasser-eddine Monir](https://nasseredd.github.io/teaching/) (CC BY-NC-SA) -- 2025\n",
    "\n",
    "\n",
    "* üë• You may work in pairs,\n",
    "* üì© however, submit your work individually by the end of the class.\n",
    "* üìù Your names should be included in the file names as follows: **Practical-3-Monir.ipynb**.\n",
    "* üìß Please ensure you email me EXCLUSIVELY at the following address: nasser-eddine.monir@inria.fr\n",
    "* üí¨ Don't forget to leave a comment whenever an observation is requested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Steps\n",
    "\n",
    "1. Change the run time and choose a **T4 GPU**.\n",
    "2. Click on \"Upload to session storage\" button to load ```data.zip```.\n",
    "3. Unzip the folder ```!unzip /content/data.zip -d /content/data```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Install ```museval``` using ```pip```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Install these packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/espnet/espnet\n",
    "%pip install -q espnet_model_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import librosa\n",
    "import soundfile\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from espnet2.bin.enh_inference import SeparateSpeech\n",
    "from espnet_model_zoo.downloader import ModelDownloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Signals\n",
    "\n",
    "This function, ```load_signals```, is designed to load and return speech, noise, and their corresponding Room Impulse Responses (RIRs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signals(speech_file, noise_file, speech_rir_file, noise_rir_file):\n",
    "    # Load the speech signal\n",
    "    speech_signal, _ = librosa.load(speech_file, sr=16000)\n",
    "\n",
    "    # Load the noise signal\n",
    "    noise_signal, _ = librosa.load(noise_file, sr=16000)\n",
    "\n",
    "    # Load the RIRs\n",
    "    speech_rir_data = np.load(speech_rir_file)\n",
    "    noise_rir_data = np.load(noise_rir_file)\n",
    "\n",
    "    # Extract the RIR signals\n",
    "    speech_rir = speech_rir_data['rir']\n",
    "    noise_rir = noise_rir_data['rir']\n",
    "\n",
    "    return speech_signal, noise_signal, speech_rir, noise_rir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixture\n",
    "\n",
    "Create mixtures by assigning a unique noise signal to each speech signal while maintaining the following conditions:\n",
    "\n",
    "- Speech Position: Always at the front (0¬∞).\n",
    "- Noise Position: Always at 90¬∞ to the right.\n",
    "- Signal-to-Noise Ratio (SNR): Fixed at 0 dB.\n",
    "\n",
    "Ensure that each speech signal is paired with exactly one noise signal, resulting in distinct mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Enhancement\n",
    "\n",
    "The next three cells are from the ESPnet tutorial on speech enhancement algorithms, available on their [GitHub repository](https://github.com/espnet/espnet). It offers four pretrained models, including MVDR and FaSNet. Choose one, run the cells carefully, then visualize and listen to the generated mixture and estimated speech.\n",
    "\n",
    "The models processe a **four-channel mixture**, selects a **reference channel**, and outputs a **single-channel estimated speech**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 16000 #@param {type:\"integer\"}\n",
    "tag = \"espnet/Wangyou_Zhang_chime4_enh_train_enh_beamformer_mvdr_raw\" #@param [\"espnet/Wangyou_Zhang_chime4_enh_train_enh_beamformer_mvdr_raw\", \"espnet/Wangyou_Zhang_chime4_enh_train_enh_dc_crn_mapping_snr_raw\", \"lichenda/chime4_fasnet_dprnn_tac\", \"https://zenodo.org/record/6025881/files/enh_train_enh_beamformer_mvdr_raw_valid.si_snr.ave.zip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ModelDownloader()\n",
    "\n",
    "cfg = d.download_and_unpack(tag)\n",
    "enh_model_mc = SeparateSpeech(\n",
    "  train_config=cfg[\"train_config\"],\n",
    "  model_file=cfg[\"model_file\"],\n",
    "  normalize_segment_scale=False,\n",
    "  show_progressbar=True,\n",
    "  ref_channel=4,\n",
    "  normalize_output_wav=True,\n",
    "  device=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1SmrN5NFSg6JuQSs2sfy3ehD8OIcqK6wS -O /content/M05_440C0213_PED_REAL.wav\n",
    "mixwav_mc, sr = soundfile.read(\"/content/M05_440C0213_PED_REAL.wav\") # mixwav.shape: num_samples, num_channels\n",
    "mixwav_sc = mixwav_mc[:,4]\n",
    "wave = enh_model_mc(mixwav_mc[None, ...], sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a model and use it to generate the estimated speech from the chosen mixture. Once processed, listen to the output to evaluate the quality of the enhanced speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're more familiar with speech enhancement using ESPnet, create a function that processes a given **mixture folder**, selects the **first channel as the reference**, and generates the **estimated speech** in the `est_speech/` folder using FaSNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code me!\n",
    "def inference():\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Museval is a library for evaluating speech enhancement models by computing key performance metrics such as SDR, SIR, and SAR.\n",
    "\n",
    "In speech enhancement, we evaluate the estimated speech by comparing it to the clean reference and noise. Your task is to stack the **reference sources** (clean speech and noise) and the corresponding **estimates** as follows:\n",
    "\n",
    "Given:\n",
    "- **$s$** = clean speech (target signal)\n",
    "- **$n$** = noise\n",
    "- **$ \\hat{s} $** = estimated speech\n",
    "\n",
    "Stack the reference sources and estimates as:\n",
    "\n",
    "$\\text{references} = \\begin{bmatrix}\n",
    "s \\\\\n",
    "n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\n",
    "\\text{estimates} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{s} \\\\\n",
    "\\hat{s} - s\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Next, use the `bss_eval` function from `museval.metrics` to compute the enhancement metrics.  \n",
    "Set `filters_len=1` to ensure proper evaluation.  \n",
    "\n",
    "Run the evaluation on **one example of your choice**, print SIR, SAR and SDR, and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Signal-to-Interference Ratio** (**SIR**) measures how well the target speech signal is separated from interfering noise or other unwanted sources. A higher SIR indicates better suppression of interference while preserving the target speech signal.\n",
    "\n",
    "* **Signal-to-Artifacts Ratio** (**SAR**) quantifies the amount of distortion or artifacts introduced during signal processing. A higher SAR means fewer processing artifacts, ensuring the recovered signal remains natural and undistorted.\n",
    "\n",
    "* **Signal-to-Distortion Ratio** (**SDR**) is an overall measure of signal quality that combines interference suppression and artifact minimization. A higher SDR indicates a better-quality reconstructed signal, balancing both interference removal and minimal processing distortions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlay the spectrums of the speech signal, the noisy mixture, and the estimated speech on the same plot using three distinct colors. This visualization will help compare the spectral differences and assess the effectiveness of the enhancement process. \n",
    "\n",
    "**Note**: Ensure that frequencies and magnitudes are clamped within the range of 50 to 5000 Hz to focus on the most relevant speech frequencies. Additionally, display both the x-axis (frequency) and y-axis (magnitude) in logarithmic scale for better visualization of spectral variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average values of SIR, SAR, and SDR across the entire corpus to evaluate the overall performance of the separation system. This will provide a global assessment of interference suppression, artifact reduction, and signal quality across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the MVDR model to generate inferences on the same set of mixtures. Compute the evaluation metrics (SIR, SAR, SDR) and compare the results with the previous estimations obtained using the FaSNet model to assess their relative performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus (+3pts on Your Practicals Grade)\n",
    "\n",
    "This part is optional and can be done at home until Friday, February 13th, at 11:59 PM.\n",
    "\n",
    "By this deadline, you should:\n",
    "\n",
    "- Use an ASR model (e.g., wav2vec, Whisper) to generate transcriptions of the clean speech.\n",
    "- Apply automatic phoneme segmentation using Montreal Forced Aligner (MFA).\n",
    "- Select a specific phoneme category (e.g., plosives, fricatives) and concatenate all occurrences of this category in the clean, mixture, and estimated speech (from one algorithm) to create three signals.\n",
    "- Compute evaluation metrics comparing the clean and estimated speech of this phoneme category.\n",
    "- Overlay and visualize the spectrums of the three signals.\n",
    "- Write a brief analysis summarizing the results of this experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
